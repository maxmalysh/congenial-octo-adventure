# 1.
# Реализовать предобусловленный метод сопряженных градиентов для систем с матрицами Стилтьеса
# с предобуславливанием по методам ILU(k), MILU(k) и ILU(k,e)
#
# (в последнем случае речь идёт об алгоритме ILU(k), в котором портрет матрицы заменён на множетсов пар индексов,
#  включающее пары равных индексов и пары индексов коэффициентов матрицы по модулю больших e);
#
# провести анализ скорости сходимости для заданной системы и подобрать приемлемые значения k.
#

#
# стр. 90 – Метод сопряжённых градиентов
# стр. 102 - Предобусловленный метод сопряженных градиентов
# стр. 112 – Предобуславливание с использованием неполного LU-разложения
# стр. 113 – определение матриц Стилтьеса
# стр. 119 – определение ILU(k) разложения
# стр. 120 – MILU-разложение
#
# Предобуславливание (также предобусловливание) — процесс преобразования условий задачи
# для её более корректного численного решения.
#
# Предобуславливание обычно связано с уменьшением числа обусловленности задачи.
# Предобуславливаемая задача обычно затем решается итерационным методом.
#
# https://ru.wikipedia.org/wiki/Метод_сопряжённых_градиентов
# Метод сопряженных градиентов — метод нахождения локального экстреммума функции
# на основе информации о её значениях и её градиенте.
#
#
#
#

import numpy as np


# Golub, van Loun 4rd ed., p. 632
def conj_gradients_method(A: np.matrix, b: np.ndarray, x_0: np.ndarray):
    x = {};
    r = {};
    beta = {};
    alpha = {};
    q = {};
    c = {};
    d = {};
    nu = {};
    l = {};
    x[0] = x_0
    r[0] = b - A @ x_0
    beta[0] = np.linalg.norm(r[0], ord=2)
    q[0] = 0
    c[0] = 0

    k = 0
    while beta[k] != 0:
        q[k + 1] = r[k] / beta[k]
        k += 1
        alpha[k] = q[k].transpose() * A * q[k]

        if k == 1:
            d[1] = alpha[1];
            nu[1] = beta[0] / d[1]
            c[k] = q[1]
        else:
            l[k - 1] = beta[k - 1] / d[k - 1];
            d[k] = alpha[k] - beta[k - 1] * l[k - 1];
            nu[k] = -beta[k - 1] * nu[k - 1] / d[k]
            c[k] = q[k] - l[k - 1] * c[k - 1]

        x[k] = x[k - 1] + nu[k] * c[k]
        r[k] = A * q[k] - alpha[k] * q[k] - beta[k - 1] * q[k - 1]
        beta[k] = np.linalg.norm(r[k], ord=2)
        print(k, x[k])

    x_star = x[k]
    return x_star


if __name__ == "__main__":
    A = np.array([[1, 3],
                  [3, 1]])
    b = np.array([1, 2])
    x_0 = np.array([5.5 / 8, 1 / 8])
    x_star = conj_gradients_method(A, b, x_0)
    print(x_star)
